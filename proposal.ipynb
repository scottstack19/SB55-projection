{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science â€“ Project\n",
    "*COMP 5360 / MATH 4100, University of Utah, http://datasciencecourse.net/*\n",
    "\n",
    "Project Super Bowl 55 winner based on in season statistics Correlation study on offense/defense statistics to determine key prediction factors. Use machine learning to predict final standings of playoffs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Data\n",
    "First Name: Scott\n",
    "<br>  \n",
    "Last Name: Stack\n",
    "<br>  \n",
    "E-mail: u0664997@utah.edu\n",
    "<br>  \n",
    "UID:  u0664997\n",
    "<br> \n",
    "\n",
    "First Name: Will\n",
    "<br>  \n",
    "Last Name: Gilliland\n",
    "<br>  \n",
    "E-mail: u0680231@utah.edu\n",
    "<br>  \n",
    "UID:  u0680231\n",
    "<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Motivation. \n",
    "\n",
    "Discuss your motivations and reasons for choosing this project, especially any background or research interests that may have influenced your decision.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Objectives. \n",
    "Provide the primary questions you are trying to answer in your project. What would you like to learn and accomplish? List the benefits.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data. \n",
    "The data for the project will be obtained from [Pro Football Focus](https://www.pro-football-reference.com/), specifically the pages containing data from each year, i.e., [2007](https://www.pro-football-reference.com/years/2007/). Pages for each year readily contain team-based stats for the year as well as conference standings and results, which is the data that will be used to create models to make predictions. The intended method to obtain data will to be to scrape the site using the libraries [urllib](https://docs.python.org/3/library/urllib.html) to create HTML files for each page and [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) to gather relevent data from each url. The site is structured such that a for-loop can easily be written to collect data over a desired range of years.\n",
    "\n",
    "Based on the site's [terms of use](https://www.sports-reference.com/termsofuse.html) section 5. Permitted Use subsections (i) and (j):\n",
    "- \"(i) without our express written permission, use any automated means to access or use the Site, including scripts, bots, scrapers, data miners, or similar software, in a manner that adversely impacts site performance or access; or\"\n",
    "- (j) use any material or Content from the Site, including without limitation any statistics or data, (i) to create any database, archive, or other data store that competes with or constitutes a material substitute for the services or data stores offered on the Site or (ii) to provide any service that competes with or constitutes a material substitute for the services or data stores offered on the Site\"\n",
    "\n",
    "From these statements, the intended use for the project to quickly scrape data in a matter that will not slow traffic, makes changes to, or compete with the site is allowed without express permission.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethical considerations. \n",
    "Complete a stakeholder analysis for your project.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing. \n",
    "Do you expect to do substantial data cleanup or data extraction? What quantities do you plan to derive from your data? How will data processing be implemented?\n",
    "\n",
    "Substantial data extraction methods should not be required. The HTML for each page is well organized with tabulated data that is easily accessed, as seen below. Extraction should only consist of creating an HTML for each url, parsing the HTML, and reading the desired tables into a data frame. Additionally after the data is extracted it will be saved to a csv file to use to reduce on the time spent extracting data and to reduce the use of the data-scraping script to better adhere to the terms of service of the site stated above. Cleanup of the data should not be intensive as the chosen data sets are extremely well documented.\n",
    "\n",
    "![](example_html1.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Exploratory Analysis.\n",
    "The data sets that will be used will contain many variables and it will be important to explore these to find which variables are predictive. The main exploratory visualization will be the use of a correlation matrix with further exploration through a scatterplot matrix. The results from this exploration should allow the removal of variables that do not appear to be predictive. This will allow for the creation of more accurate models used for analysis. Further exploration of these variables will be completed using a regression and predictiveness decided on the $R^2$ value for a regression model and the p-values for each variable.\n",
    "\n",
    "Further visualization will be in the form of a classifcation cluster with decision boundaries. With a prediction that is binary, either a team wins the superbowl or they don't, this visualization method should allow for clear visualization of the accuracy of a model's prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Methodology. \n",
    "How are you planning to analyze your data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Schedule. \n",
    "Make sure that you plan your work so that you can avoid a big rush right before the final project deadline, and delegate different modules and responsibilities among your team members. Write this in terms of weekly deadlines.\n",
    "\n",
    "**Week 1:**\n",
    "\n",
    "- Create script to scrape the required data\n",
    "     - Create HTML from url\n",
    "     - Parse HTML and extract data\n",
    "     - Save data to csv for ease of use\n",
    "- Begin exploratory analysis\n",
    "\n",
    "**Week 2:**\n",
    "\n",
    "- Exploratory analysis\n",
    "    - Correlation matrix\n",
    "    - Visualization of corelation with scatterplot matrix\n",
    "- Decide which variables are predictive based on exploratory analysis\n",
    "- Compile results of exploratory analysis into document\n",
    "- Milestone due (April 11, 2021)\n",
    "\n",
    "**Week 3:**\n",
    "\n",
    "- Begin Analysis\n",
    "    - Classification\n",
    "        - Split data\n",
    "        - Find parameter values that produce most accurate results\n",
    "        - Create model based on optimal parameters\n",
    "        - Train model\n",
    "        - Predict\n",
    "        - Visualization\n",
    "    - Method 2\n",
    "        \n",
    "\n",
    "**Week 4:**\n",
    "\n",
    "- Finish Analysis\n",
    "- Compile Analysis results into document\n",
    "- Editing final submission\n",
    "- Project due (April 25, 2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
